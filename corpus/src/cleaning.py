from inspect import cleandoc
import unicodedata
from pkg_resources import yield_lines
from tqdm import tqdm
import re
import regex
import multiprocessing as mp
import os


def is_en(sents):
    en = re.compile("""[a-zA-Z   # ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆ
                        0-9      # ã‚¢ãƒ©ãƒ“ã‚¢æ•°å­—
                        \u2160-\u2188   # ãƒ­ãƒ¼ãƒæ•°å­—
                        \u0020-\u002F\u003A-\u0040\u005B-\u0060\u007B-\u007E   # ASCIIè¨˜å·ã®åŠè§’ç‰ˆ
    ]+""")

    en_tf_ls = []
    for sent in sents:
        if en.fullmatch(sent) is None:
            en_tf_ls.append(False)
        else:
            en_tf_ls.append(True)
    return en_tf_ls


def is_ja(sents):
    ja = regex.compile("""[\u3041-\u309F                    # ã²ã‚‰ãŒãª
                            \u30A1-\u30FF\uFF66-\uFF9F      # ã‚«ã‚¿ã‚«ãƒŠ
                            0-9ï¼-ï¼™                        # ã‚¢ãƒ©ãƒ“ã‚¢æ•°å­—
                            \p{Numeric_Type=Numeric}        # æ¼¢æ•°å­—ã€ãƒ­ãƒ¼ãƒæ•°å­—
                            \p{Script_Extensions=Han}       # æ¼¢å­—
                            # ASCIIæ–‡å­—(è¨˜å·)ã®åŠè§’ç‰ˆ
                            \u0020-\u002F\u003A-\u0040\u005B-\u0060\u007B-\u007E
                            # ASCIIæ–‡å­—(è¨˜å·)å…¨è§’ç‰ˆã¨æ—¥æœ¬èªã®è¨˜å·ã®åŠè§’ç‰ˆ
                            \uFF01-\uFF0F\uFF1A-\uFF20\uFF3B-\uFF40\uFF5B-\uFF65\u3000-\u303F
    ]+""")

    ja_tf_ls = []
    for sent in sents:
        if ja.fullmatch(sent) is None:
            ja_tf_ls.append(False)
        else:
            ja_tf_ls.append(True)
    return ja_tf_ls


def rm_noise(en_sents, ja_sents, en_q, ja_q):
    """
    æ­£è¦è¡¨ç¾ã‚’ç”¨ã„ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å«ã¾ã‚Œã‚‹ãƒã‚¤ã‚º(è¨˜å·, URL, ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹, etc...)ã‚’é™¤å»ã™ã‚‹é–¢æ•°
    é«˜é€ŸåŒ–ã®ãŸã‚ã€æ­£è¦è¡¨ç¾ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’äº‹å‰ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦ãŠãã€‚

    åŒã˜æ©Ÿèƒ½ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã®æ­£è¦è¡¨ç¾ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ä¸€é€šã‚Šã§ã¯ãªãã€ã„ãã¤ã‚‚è€ƒãˆã‚‰ã‚Œã‚‹ã€‚
    ã—ã‹ã—ã€ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã£ã¦ã¯ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’æ„å›³ã›ãšåœæ­¢ã•ã›ã¦ã—ã¾ã†ã“ã¨ãŒã‚ã‚‹ã‹ã‚‰ã€
    æ–°ã—ã„æ©Ÿèƒ½ã‚’ã“ã®é–¢æ•°ã«è¿½åŠ ã™ã‚‹ã¨ãã¯ã€ååˆ†ã«ãƒ†ã‚¹ãƒˆã™ã‚‹ã€‚
    """
    brackets = re.compile(r"""\<.*?\>|\{.*?\}|\(.*?\)|\[.*?\]|   # æ‹¬å¼§ï¼ˆåŠè§’ï¼‰
                            |ã€.*?ã€‘|ï¼ˆ.*?ï¼‰|ã€ˆ.*?ã€‰|ã€Š.*?ã€‹|ã€Œ.*?ã€|ã€.*?ã€|ã€.*?ã€‘|                # æ‹¬å¼§ï¼ˆå…¨è§’ï¼‰
                            |ã€”.*?ã€•|ã€–.*?ã€—|ã€˜.*?ã€™|ã€š.*?ã€›|ï½›.*?ï½|ï¼œ.*?ï¼|ï½›.*?ï½|ï½Ÿ.*?ï½ |ï¼œ.*?ï¼  # æ‹¬å¼§ï¼ˆå…¨è§’ï¼‰
                            """)
    unwanted = re.compile(
        r"[*#^\ã€Œ\ã€\ã€\ã€\ã€ˆ\ã€‰:;\<\>\{\}\"\(\)\[\]]+")   # é–“é•ã£ã¦ ç©ºç™½ã‚’å…¥ã‚Œã¦ã—ã¾ã‚ãªã„ã‚ˆã†ã«æ³¨æ„ã™ã‚‹
    msc = re.compile(r"\\\\|\t|\\\\t|\r|\\\\r")
    newlines = re.compile(r"\\\n|\n")
    urls = re.compile(
        r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+")
    email = re.compile(
        r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b"
    )
    encoding_err = re.compile("0000,0000,0000,\w*?")
    multi_space = re.compile("[ ã€€]{2,}")
    emoji = regex.compile("\p{Emoji_Presentation=Yes}+")
    hiragana_rare = re.compile(
        "[\U0001B001-\U0001B11F\U0001B150-\U0001B152\U0001F200]+")
    katakana_rare = re.compile(
        "[\u31F0-\u31FF\u32D0-\u32FE\u3300-\u3357\U0001AFF0-\U0001AFFE\U0001B000\U0001B120-\U0001B122\U0001B164-\U0001B167]+")

    cleaned_en, cleaned_ja = [], []
    print("Start denoising sentences... (Process ID: {})".format(os.getpid()))

    for en_sent, ja_sent in zip(en_sents, ja_sents):
        en_sent = unicodedata.normalize("NFKC", en_sent).strip()
        ja_sent = unicodedata.normalize("NFKC", ja_sent).strip()
        en_sent = urls.sub('', en_sent)
        ja_sent = urls.sub('', ja_sent)

        en_sent = email.sub('', en_sent)
        ja_sent = email.sub('', ja_sent)

        en_sent = msc.sub(' ', en_sent)
        ja_sent = msc.sub(' ', ja_sent)

        en_sent = newlines.sub('', en_sent)
        ja_sent = newlines.sub('', ja_sent)

        en_sent = emoji.sub('', en_sent)
        ja_sent = emoji.sub('', ja_sent)

        en_sent = brackets.sub('', en_sent)
        ja_sent = brackets.sub('', ja_sent)

        en_sent = unwanted.sub('', en_sent)
        ja_sent = unwanted.sub('', ja_sent)

        ja_sent = hiragana_rare.sub('', ja_sent)
        ja_sent = katakana_rare.sub('', ja_sent)

        en_sent = multi_space.sub(' ', en_sent)
        ja_sent = multi_space.sub(' ', ja_sent)

        en_sent = encoding_err.sub('', en_sent)
        ja_sent = encoding_err.sub('', ja_sent)

        cleaned_en.append(en_sent.strip())
        cleaned_ja.append(ja_sent.strip())

    en_q.put(cleaned_en)
    ja_q.put(cleaned_ja)

    print("Finished denoising sentences... (Process ID: {})".format(os.getpid()))


def clean(en_sents, ja_sents, workers=1):
    """
    æ­£è¦è¡¨ç¾ã‚’ç”¨ã„ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å«ã¾ã‚Œã‚‹å„ç¨®ã®ãƒã‚¤ã‚º(è¨˜å·, URL, ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹, etc...)ã‚’é™¤å»ã™ã‚‹ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿é–¢æ•°
    ã¾ãŸã€æ—¥è‹±ä»¥å¤–ã®è¨€èªã®æ–‡ã‚‚ç™ºè¦‹æ¬¡ç¬¬é™¤å»ã™ã‚‹ã€‚
    ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹å¯¾å¿œæ¸ˆã¿(å¼•æ•° workers ã‚’ç”¨ã„ã¦ãƒ—ãƒ­ã‚»ã‚¹æ•°ã‚’æŒ‡å®šã™ã‚‹)
    """
    min_workers = 1
    max_workers = 8
    num_sents = min(len(en_sents), len(ja_sents))
    workers = 1 if workers < min_workers or workers > max_workers or workers > num_sents else workers
    size = int(num_sents/workers)

    en_q = mp.Queue()
    ja_q = mp.Queue()

    tgt_fun = rm_noise
    for idx in range(workers):
        head = idx * size
        tail = (idx+1) * size if idx != (workers-1) else num_sents
        proc = mp.Process(target=tgt_fun, args=[
                          en_sents[head:tail], ja_sents[head:tail], en_q, ja_q])
        proc.start()

    cleaned_en, cleaned_ja = [], []
    for _ in range(workers):
        for en_sent, ja_sent in zip(en_q.get(), ja_q.get()):
            cleaned_en.append(en_sent)
            cleaned_ja.append(ja_sent)

    print("\nChecking if downloaded sentences are truly English or Japanese sentences...")

    en_ls, ja_ls = [], []
    for idx, (en_tf, ja_tf) in enumerate(zip(is_en(cleaned_en), is_ja(cleaned_ja))):
        if en_tf and ja_tf:
            en_ls.append(cleaned_en[idx])
            ja_ls.append(cleaned_ja[idx])

    return en_ls, ja_ls


# ãƒ†ã‚¹ãƒˆç”¨ã‚³ãƒ¼ãƒ‰
if __name__ == "__main__":
    # cleané–¢æ•°å…¨ä½“ã®ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰
    en_sents = [
        "the college ğŸ¤©provided courses in science , engineering and art , and also established its own internal degree courses in science and engineering , which were ratified by the university of london .",
        "he won the \t fossati prize . \n https://en.wikipedia.org/wiki/Giampiero_Fossati https://www.w3resource.com/python-exercises/re/python-re-exercise-42.php",
        "he was born in haarlem as the son of aart jansz and became a lawyer .",
        "He said (ç§ã¯æ—¥ç³»ã‚¢ãƒ¡ãƒªã‚«äººäºŒä¸–ã§ã™ã€‚).",
        "I    have a ğŸ’¯(pen). timo.33@gmail.com"
    ]

    ja_sents = [
        "å½“æ™‚ ã® ğŸ˜€ğŸ˜ƒå­¦æ ¡ ã¯ ç§‘å­¦ ã€ å·¥å­¦ ã€ ğŸ¤©èŠ¸è¡“ ã® ã‚³ãƒ¼ã‚¹ ã‚’ é–‹è¬› ã— ã€ ãã‚Œ ã‚‰ ã® ã‚³ãƒ¼ã‚¹ ã¯ ãƒ­ãƒ³ãƒ‰ãƒ³ å¤§å­¦ ã‚ˆã‚Š å­¦ä½ ã® æ‰¿èª ã‚’ å—ã‘ ã¦ ã„ ãŸ ã€‚",
        "^ ãƒ”ã‚¿ãƒªè³ ã‚’ ç²å¾— ã— ãŸ ã€‚http://en.wikipedia.org/wiki/Giampiero_Fossati",
        "er wurde in haarlem als sohn von aart jansz geboren und wurde rechtsanwalt.",
        "å½¼ã¯ã€Œæ—¥ç³»ã‚¢ãƒ¡ãƒªã‚«äººäºŒä¸–ã§ã™ã€‚ï¼IVã€ã¨è¨€ã£ãŸã€‚",
        "ç§ã¯ãƒšãƒ³ã€€ã€€ã‚’æŒã£ã¦ã„ã¾ã™ã€‚*ğŸ’¯"
    ]

    # is_ené–¢æ•°ã¨is_jaé–¢æ•°ã®ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰
    #en_tf_ls, ja_tf_ls = [], []
    # for en_tf, ja_tf in zip(is_en(en_sents), is_ja(ja_sents)):
    #  en_tf_ls.append(en_tf)
    #  ja_tf_ls.append(ja_tf)
    # print(en_tf_ls)
    # print(ja_tf_ls)

    # cleané–¢æ•°å…¨ä½“ã®ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰

    en_ls, ja_ls = clean(en_sents=en_sents, ja_sents=ja_sents, workers=4)
    print(en_ls)
    print(ja_ls)

    # rm_noiseé–¢æ•°ã§ç”¨ã„ã‚‰ã‚Œã¦ã„ã‚‹æ­£è¦è¡¨ç¾ã®ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰
    # en = "^(Hello Nice to meet# you.) ljsadfjl:kashttps://www.w3resource.com/python-exercises/re/python-re-exercise-42.php"
    # ja = "ã‚„***ã‚**ï¼œã“ã‚“ã« ğ›€ã¡ã¯ï¼^#ã€ã¿ã‚“ãªã€ãƒã‚±ãƒ¢ãƒ³<>080- ã‹ 	ãˆ 4482-1811;ğ›€‚  ğ›€ƒ https://en.wikipedia.org/wiki/Giampiero_Fossati "
    # hiragana_rare = re.compile(
    #    "[\U0001B001-\U0001B11F\U0001B150-\U0001B152\U0001F200]+")
    # katakana_rare = re.compile(
    #    "[\u31F0-\u31FF\u32D0-\u32FE\u3300-\u3357\U0001AFF0-\U0001AFFE\U0001B000\U0001B120-\U0001B122\U0001B164-\U0001B167]+")
    #urls = re.compile("http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+")
    #cleaned_en = hiragana_rare.sub('', en)
    #cleaned_ja = hiragana_rare.sub('', ja)
    #cleaned_en = katakana_rare.sub('', en)
    #cleaned_ja = katakana_rare.sub('', ja)
    #cleaned_en = urls.sub('', en)
    #cleaned_ja = urls.sub('', ja)
    #print("BF: {}\nAF: {}".format(en, cleaned_en))
    #print("BF: {}\nAF: {}".format(ja, cleaned_ja))
